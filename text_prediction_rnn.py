# -*- coding: utf-8 -*-
"""
7_Text_Prediction_RNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pjA0o9PtRzDqMMrLgJlBXpSOBYAZhB4u
"""

# Install necessary libraries (uncomment if running this in a new environment)
# !pip install contractions emoji
# !pip install tensorflow
import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'

import re
import contractions
import emoji
import nltk

# Download necessary NLTK data
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
import pickle
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences


# Initialize stopwords and lemmatizer
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    """
    Preprocesses the input text by:
    - Lowercasing
    - Expanding contractions
    - Removing URLs
    - Removing mentions, hashtags, and emojis
    - Removing special characters and digits
    - Tokenizing and removing stopwords
    - Lemmatizing words
    """
    text = text.lower()  # Lowercasing
    text = contractions.fix(text)  # Expanding contractions
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)  # Removing URLs
    text = re.sub(r'\@\w+|\#', '', text)  # Removing mentions and hashtags
    text = emoji.demojize(text)  # Converting emojis to text
    text = re.sub(r'_', ' ', text)  # Removing underscore from emoji descriptions
    text = re.sub(r'[^\w\s]', '', text)  # Removing special characters
    text = re.sub(r'\d+', '', text)  # Removing digits
    text = word_tokenize(text)  # Tokenizing
    text = ' '.join([word for word in text if word not in stop_words])  # Removing stopwords
    text = ' '.join([lemmatizer.lemmatize(word) for word in text])  # Lemmatization
    return text

# Load the trained model
model = load_model('rnn_model.keras')

# Load the tokenizer
with open('tokenizer_rnn.pkl', 'rb') as f:
    tokenizer = pickle.load(f)

def predict_cyberbullying(text):
    """
    Predicts whether the given text indicates cyberbullying or not.
    """
    # Preprocess the input text
    cleaned_text = preprocess_text(text)
    
    # Convert the cleaned text into a sequence and pad it
    text_sequence = tokenizer.texts_to_sequences([cleaned_text])
    padded_text = pad_sequences(text_sequence, maxlen=100, padding='pre', truncating='post')

    # Make a prediction using the model
    prediction = model.predict(padded_text)

    # Return the prediction result
    return "Cyberbullying" if prediction[0] > 0.5 else "Not Cyberbullying"

# Test the function in a standalone script
if __name__ == '__main__':
    text = input("Enter The Comment: ")
    print(predict_cyberbullying(text))
